:py:mod:`anomalib.models.stfpm`
===============================

.. py:module:: anomalib.models.stfpm

.. autoapi-nested-parse::

   STFPM Model.



Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   anomaly_map/index.rst
   lightning_model/index.rst
   loss/index.rst
   torch_model/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   anomalib.models.stfpm.Stfpm
   anomalib.models.stfpm.StfpmLightning




.. py:class:: Stfpm(input_size: Tuple[int, int], backbone: str, layers: List[str])

   Bases: :py:obj:`anomalib.models.components.AnomalyModule`

   PL Lightning Module for the STFPM algorithm.

   :param input_size: Size of the model input.
   :type input_size: Tuple[int, int]
   :param backbone: Backbone CNN network
   :type backbone: str
   :param layers: Layers to extract features from the backbone CNN
   :type layers: List[str]

   .. py:method:: training_step(self, batch, _)

      Training Step of STFPM.

      For each batch, teacher and student and teacher features are extracted from the CNN.

      :param batch: Input batch
      :type batch: Tensor
      :param _: Index of the batch.

      :returns: Hierarchical feature map


   .. py:method:: validation_step(self, batch, _)

      Validation Step of STFPM.

      Similar to the training step, student/teacher features are extracted from the CNN for each batch, and
      anomaly map is computed.

      :param batch: Input batch
      :type batch: Tensor
      :param _: Index of the batch.

      :returns: Dictionary containing images, anomaly maps, true labels and masks.
                These are required in `validation_epoch_end` for feature concatenation.



.. py:class:: StfpmLightning(hparams: Union[omegaconf.DictConfig, omegaconf.ListConfig])

   Bases: :py:obj:`Stfpm`

   PL Lightning Module for the STFPM algorithm.

   :param hparams: Model params
   :type hparams: Union[DictConfig, ListConfig]

   .. py:method:: configure_callbacks(self)

      Configure model-specific callbacks.

      .. note::

         This method is used for the existing CLI.
         When PL CLI is introduced, configure callback method will be
             deprecated, and callbacks will be configured from either
             config.yaml file or from CLI.


   .. py:method:: configure_optimizers(self) -> torch.optim.Optimizer

      Configures optimizers for each decoder.

      .. note::

         This method is used for the existing CLI.
         When PL CLI is introduced, configure optimizers method will be
             deprecated, and optimizers will be configured from either
             config.yaml file or from CLI.

      :returns: Adam optimizer for each decoder
      :rtype: Optimizer



