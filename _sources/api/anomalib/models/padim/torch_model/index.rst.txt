:py:mod:`anomalib.models.padim.torch_model`
===========================================

.. py:module:: anomalib.models.padim.torch_model

.. autoapi-nested-parse::

   PyTorch model for the PaDiM model implementation.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   anomalib.models.padim.torch_model.PadimModel




Attributes
~~~~~~~~~~

.. autoapisummary::

   anomalib.models.padim.torch_model.DIMS


.. py:data:: DIMS
   

   

.. py:class:: PadimModel(input_size: Tuple[int, int], layers: List[str], backbone: str = 'resnet18', pre_trained: bool = True)

   Bases: :py:obj:`torch.nn.Module`

   Padim Module.

   :param input_size: Input size for the model.
   :type input_size: Tuple[int, int]
   :param layers: Layers used for feature extraction
   :type layers: List[str]
   :param backbone: Pre-trained model backbone. Defaults to "resnet18".
   :type backbone: str, optional
   :param pre_trained: Boolean to check whether to use a pre_trained backbone.
   :type pre_trained: bool, optional

   .. py:method:: forward(self, input_tensor: torch.Tensor) -> torch.Tensor

      Forward-pass image-batch (N, C, H, W) into model to extract features.

      :param input_tensor: Image-batch (N, C, H, W)
      :param input_tensor: Tensor:

      :returns: Features from single/multiple layers.

      .. rubric:: Example

      >>> x = torch.randn(32, 3, 224, 224)
      >>> features = self.extract_features(input_tensor)
      >>> features.keys()
      dict_keys(['layer1', 'layer2', 'layer3'])

      >>> [v.shape for v in features.values()]
      [torch.Size([32, 64, 56, 56]),
      torch.Size([32, 128, 28, 28]),
      torch.Size([32, 256, 14, 14])]


   .. py:method:: generate_embedding(self, features: Dict[str, torch.Tensor]) -> torch.Tensor

      Generate embedding from hierarchical feature map.

      :param features: Hierarchical feature map from a CNN (ResNet18 or WideResnet)
      :type features: Dict[str, Tensor]

      :returns: Embedding vector



